{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761bef56-6371-4bff-b0de-a91fe371e7d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8ce06a-1c05-4763-a02f-bc20620c88d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Functions & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99a359d-0033-4a57-b09b-f933529948c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from Tools.leica_tools import RawLoader\n",
    "from Tools.db_tools import DbManager\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from keras.api.optimizers import Adam\n",
    "from keras.api.models import Model, load_model\n",
    "from keras.api.layers import Input, Conv2D, Flatten, Dense, MaxPooling2D, Dropout\n",
    "from keras.api.losses import CategoricalCrossentropy\n",
    "from keras.api.metrics import CategoricalAccuracy\n",
    "from keras.api.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cell count classification"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1eda3bc46ae2b697"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def cell_count(inputs, cls_label):\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', name=cls_label + '_conv1')(inputs)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2), name=cls_label + '_pool1')(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', name=cls_label + '_conv2')(pool1)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2), name=cls_label + '_pool2')(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', name=cls_label + '_conv3')(pool2)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2), name=cls_label + '_pool3')(conv3)\n",
    "\n",
    "    flatten = Flatten(name=cls_label + '_flatten')(pool3)\n",
    "\n",
    "    dense1 = Dense(512, activation='relu', name=cls_label + '_dense1')(flatten)\n",
    "    dropout1 = Dropout(0.5, name=cls_label + '_dropout1')(dense1)\n",
    "\n",
    "    dense2 = Dense(256, activation='relu', name=cls_label + '_dense2')(dropout1)\n",
    "    dropout2 = Dropout(0.5, name=cls_label + '_dropout2')(dense2)\n",
    "\n",
    "    dense3 = Dense(128, activation='relu', name=cls_label + '_dense3')(dropout2)\n",
    "    dropout3 = Dropout(0.5, name=cls_label + '_dropout3')(dense3)\n",
    "\n",
    "    output = Dense(5, activation='softmax', name=cls_label + '_output')(dropout3)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output, name=cls_label + '_model')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Depending on how many labels need to be predicted from the droplets, several CNNs will be added to the model in parallel\n",
    "def get_model(labels):\n",
    "    input = Input(shape=(128,128,4), name='cell_count_input')\n",
    "    models = [cell_count(input, label) for label in labels]\n",
    "    model = Model(inputs=[input], outputs=[m.output for m in models])\n",
    "    model.compile(optimizer=Adam(),\n",
    "                  loss={label+'_output': CategoricalCrossentropy() for label in labels},\n",
    "                  metrics={label+'_output': CategoricalAccuracy() for label in labels})\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cadb08979cf9903b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Building the tensorflow dataset from .tfrecord files. Importantly, not all frames from a droplet database can be used. \n",
    "Only droplets that are fully annotated and were not excluded as outliers should be added to teh training dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c54d65fb56d33a2c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Build dataset from annotated data from multiple experiments\n",
    "def build_dataset(expIDs, annotation_keys):\n",
    "    dataset = dbm.get_datasets(expIDs, shuffle=True)\n",
    "    \n",
    "    # Scan through the droplets which droplets contain annotations and which droplets need to be excluded\n",
    "    filter_dfs = []\n",
    "    ann_dfs = []\n",
    "    for expID in expIDs:\n",
    "        drop_register = RawLoader(expID).get_dropregister()\n",
    "        ann_df = dbm.get_wps(expID, allow_partial=False).set_index('GlobalID').filter(annotation_keys)\n",
    "        ann_df = ann_df[ann_df.apply(lambda row: (row != 10).all(), axis=1)].copy()\n",
    "        ann_df[ann_df > 4] = 4\n",
    "\n",
    "\n",
    "        filter_df = pd.Series({ID: (ID in ann_df.index) for ID in drop_register.index}, name='include').to_frame()\n",
    "        filter_df.set_index(pd.MultiIndex.from_product([[expID], filter_df.index]), inplace=True)\n",
    "        filter_dfs.append(filter_df)\n",
    "\n",
    "        ann_df.set_index(pd.MultiIndex.from_product([[expID], ann_df.index]), inplace=True)\n",
    "        ann_dfs.append(ann_df)\n",
    "    filter_df = pd.concat(filter_dfs)\n",
    "    ann_df = pd.concat(ann_dfs)\n",
    "\n",
    "    filtered_dataset = dataset.filter(partial(filter_dataset, filter_df=filter_df))\n",
    "    annotated_dataset = filtered_dataset.map(partial(prepare_data, annotations=ann_df))\n",
    "    return annotated_dataset\n",
    "\n",
    "def filter_dataset(element, filter_df):\n",
    "    return tf.py_function(lambda x, i: filter_df.loc[(x.numpy().decode(), i.numpy()), 'include'], [element['expID'], element['GlobalID']], tf.bool)\n",
    "    \n",
    "def prepare_data(element, annotations):\n",
    "    globalID = element['GlobalID']\n",
    "    image = tf.cast(element['frame'], tf.float32)\n",
    "    image = tf.math.log(image+1)\n",
    "    image = (image - tf.reduce_min(image)) /(tf.reduce_max(image) - tf.reduce_min(image))\n",
    "    element['cell_count_input'] = image\n",
    "    \n",
    "    outputs = {}\n",
    "    for ann_key in annotations.columns:\n",
    "        label = tf.py_function(lambda x, i: annotations.loc[(x.numpy().decode(), i.numpy()), ann_key], [element['expID'], globalID], tf.int64)\n",
    "        label.set_shape(())\n",
    "        label = tf.cast(tf.one_hot(label, 5), tf.int64)\n",
    "        outputs[ann_key + '_output'] = label\n",
    "    return element, outputs\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d109eee2abf79eea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dbm = DbManager()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6ddbfbea7e50967"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "expIDs = ['NKIP_FA_052', 'NKIP_FA_055', 'NKIP_FA_056', 'FA_2024_049', 'FA_2024_050', 'AT_2024_007', ]\n",
    "annotation_keys = ['Target', 'Effector', 'dead_Target', 'dead_Effector']\n",
    "dataset = build_dataset(expIDs, annotation_keys)\n",
    "validation_dataset = build_dataset(['NKIP_FA_053'], annotation_keys)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "502ba335b86d0964"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_elements = dataset.reduce(tf.constant(0), lambda a,b: a+1).numpy()\n",
    "n_elements_val = validation_dataset.reduce(tf.constant(0), lambda a,b: a+1).numpy()\n",
    "print(f'{n_elements} frames in train dataset')\n",
    "print(f'{n_elements_val} frames in test dataset')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c485e82967bf274"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_final = dataset.repeat(12).batch(32)\n",
    "test_final = validation_dataset.repeat(12).batch(32)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "856ba1c23856b8a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = get_model(annotation_keys)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a7c4413f045cab8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_arch = plot_model(model, to_file='cell_count.png', dpi=100)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3b8ae375ce1594c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.fit(train_final, validation_data=test_final, batch_size=32, steps_per_epoch=375, epochs=12, validation_steps=29)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbb94a46c8168d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save(os.path.join(os.getenv('MODEL_DIR'), 'cell_count_v2.h5'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db8e47eaaeaaa18"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate performance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f77174d21bce748f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dbm = DbManager()\n",
    "expIDs = [ 'NKIP_FA_052', 'NKIP_FA_055', 'NKIP_FA_056', 'FA_2024_049', 'FA_2024_050', 'AT_2024_007',]\n",
    "annotation_keys = ['Target', 'Effector', 'dead_Target', 'dead_Effector']\n",
    "dataset = build_dataset(expIDs, annotation_keys)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efe2ffc46768b51e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = load_model(os.path.join(os.getenv('MODEL_DIR'), 'cell_count_v2.h5'))\n",
    "predictions = model.predict(dataset.batch(32))\n",
    "y_pred = pd.DataFrame(np.argmax(np.array(predictions), axis=-1).transpose(), columns=annotation_keys)\n",
    "\n",
    "y_true = y_pred.copy()\n",
    "for i, (element, output) in enumerate(dataset.as_numpy_iterator()):\n",
    "    y_true.iloc[i, :] = np.array([np.argmax(output[f'{key}_output']) for key in annotation_keys])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8dcbbaa102a568e0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ann2title = {'Target': 'Target',\n",
    "             'Effector': 'Effector',\n",
    "             'dead_Target': 'Dead Target',\n",
    "             'dead_Effector': 'Dead Effector'}\n",
    "\n",
    "fig,axs = plt.subplots(ncols=2, nrows=2, figsize=(4,4), dpi=100, sharey=True, sharex=True)\n",
    "for ax, ann in zip(axs.flatten(), annotation_keys):\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true=y_true[ann], y_pred=y_pred[ann], labels=np.arange(5),cmap='Blues',ax=ax,colorbar=False, normalize='true', values_format='.2f', text_kw={'fontsize': 7})\n",
    "    ax.grid(False)\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_title(ann2title[ann])\n",
    "\n",
    "axs.flatten()[0].set_ylabel('True number of cells')\n",
    "axs.flatten()[2].set_ylabel('True number of cells')\n",
    "axs.flatten()[2].set_xlabel('Predicted number of cells')\n",
    "axs.flatten()[3].set_xlabel('Predicted number of cells')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cace808ce5c8fe8e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a235b8e62e44927a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
